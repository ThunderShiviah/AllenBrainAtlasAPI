{
 "metadata": {
  "name": "",
  "signature": "sha256:de3489a47834c02fd56ad052ee3d0c13037f8c0fe50fdd161897701d7219da8f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " # Goal\n",
      " Align the example Allen SVG annotation to the example lab image.\n",
      " \n",
      " ## Discussion\n",
      " In this case, I'm trying to keep the lab image relatively unaltered and find a mapping that takes the annotation image and geometrically transforms it to something that aligns with our lab image. When I say 'align' I'm implicitly hinting at a metric. I don't currently have a metric and for this problem we probably want to be able to interchange metrics. For simplicity, my initial metric will be whether or not the alignment passes (my) visual inspection. Once I have that I can start looking at more rigorous metrics and will at least have some nice test data to optimize to. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## First Attempt\n",
      "### Overview\n",
      "[These slides](http://www.cs.utexas.edu/~grauman/courses/spring2008/slides/ShapeMatching.pdf) give a nice overview of possible techniques to match our images. What I want to do initially is find a [transformation matrix](https://en.wikipedia.org/wiki/Transformation_matrix) that will give me a simple tranformation between the allen atlas annotation and the lab image. [This website](http://franklinta.com/2014/09/08/computing-css-matrix3d-transforms/) gives a nice overview on computing css matrix 3d tranforms (a.k.a [direct linear tranformations](https://en.wikipedia.org/wiki/Direct_linear_transformation)). \n",
      "\n",
      "### Approach\n",
      "\n",
      "First I'll try to draw an ellipse around both the SVG and the lab image using a Hough tranform. Then I'll solve for the direct linear tranformation.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"document\" id=\"ellipse-detection\">\n",
      "<h1 class=\"title\">Ellipse detection</h1>\n",
      "<p>I'll try to get the ellipses around the SVG and image using ellipse detection via Hough transform.</p>\n",
      "<div class=\"section\" id=\"algorithm-overview\">\n",
      "<h1>Algorithm overview</h1>\n",
      "<p>The algorithm takes two different points belonging to the ellipse. It assumes\n",
      "that it is the main axis. A loop on all the other points determines how much\n",
      "an ellipse passes to them. A good match corresponds to high accumulator values.</p>\n",
      "<p>A full description of the algorithm can be found in reference <a class=\"footnote-reference\" href=\"#id2\" id=\"id1\">[1]</a>.</p>\n",
      "</div>\n",
      "<div class=\"section\" id=\"references\">\n",
      "<h1>References</h1>\n",
      "<table class=\"docutils footnote\" frame=\"void\" id=\"id2\" rules=\"none\">\n",
      "<colgroup><col class=\"label\" /><col /></colgroup>\n",
      "<tbody valign=\"top\">\n",
      "<tr><td class=\"label\"><a class=\"fn-backref\" href=\"#id1\">[1]</a></td><td>Xie, Yonghong, and Qiang Ji. &quot;A new efficient ellipse detection\n",
      "method.&quot; Pattern Recognition, 2002. Proceedings. 16th International\n",
      "Conference on. Vol. 2. IEEE, 2002</td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "</div>\n",
      "</div>\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cd ../data/img;ls\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "p1-E6-01b.jpg\r\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from io import BytesIO\n",
      "\n",
      "from IPython.core import display\n",
      "from PIL import Image\n",
      "\n",
      "\"\"\"displayhook function ([source](http://nbviewer.ipython.org/gist/deeplook/5162445)) in order to\n",
      "nicely display PIL image objects.\n",
      "\"\"\"\n",
      "\n",
      "def display_pil_image(im):\n",
      "   \"\"\"Displayhook function for PIL Images, rendered as PNG.\"\"\"\n",
      "\n",
      "   b = BytesIO()\n",
      "   im.save(b, format='png')\n",
      "   data = b.getvalue()\n",
      "\n",
      "   ip_img = display.Image(data=data, format='png', embed=True)\n",
      "   return ip_img._repr_png_()\n",
      "\n",
      "\n",
      "# register display func with PNG formatter:\n",
      "png_formatter = get_ipython().display_formatter.formatters['image/png']\n",
      "dpi = png_formatter.for_type(Image.Image, display_pil_image)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "from scipy import ndimage\n",
      "\n",
      "from skimage import data, color, io\n",
      "from skimage.filter import canny\n",
      "from skimage.transform import hough_ellipse\n",
      "from skimage.draw import ellipse_perimeter\n",
      "\n",
      "from PIL import Image   #for opening our image in ipython notebook.\n",
      "from IPython.display import Image as pyimg #to prevent namespace collision.\n",
      "\n",
      "# Load picture, convert to grayscale and detect edges\n",
      "\n",
      "lab_image = ndimage.imread('../data/img/p1-E6-01b.jpg')\n",
      "#lab_image = io.imread('../data/img/p1-E6-01b.jpg')\n",
      "lab_image.resize((165,118))\n",
      "lab_image"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "array([[138, 138, 138, ..., 146, 146, 144],\n",
        "       [144, 144, 144, ..., 142, 142, 142],\n",
        "       [142, 143, 143, ..., 152, 152, 152],\n",
        "       ..., \n",
        "       [181, 181, 181, ..., 179, 179, 179],\n",
        "       [179, 179, 181, ..., 175, 174, 174],\n",
        "       [174, 177, 177, ..., 173, 173, 173]], dtype=uint8)"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#image_rgb = data.coffee()[0:220, 160:420]\n",
      "#image_gray = color.rgb2gray(image_rgb)\n",
      "image_gray = lab_image\n",
      "edges = canny(image_gray, sigma=2.0,\n",
      "                     low_threshold=0.55, high_threshold=0.8)\n",
      "edges"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "array([[False, False, False, ..., False, False, False],\n",
        "       [False, False, False, ..., False, False, False],\n",
        "       [False, False, False, ..., False, False, False],\n",
        "       ..., \n",
        "       [False, False, False, ..., False, False, False],\n",
        "       [False,  True,  True, ...,  True,  True, False],\n",
        "       [False, False, False, ..., False, False, False]], dtype=bool)"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "im = lab_image\n",
      "\n",
      "# Compute the Canny filter for two values of sigma\n",
      "edges1 = canny(im)\n",
      "edges2 = canny(im, sigma=3)\n",
      "\n",
      "\n",
      "# display results\n",
      "\n",
      "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(8, 3))\n",
      "\n",
      "ax1.imshow(im, cmap=plt.cm.jet)\n",
      "ax1.axis('off')\n",
      "ax1.set_title('noisy image', fontsize=20)\n",
      "\n",
      "ax2.imshow(edges1, cmap=plt.cm.gray)\n",
      "ax2.axis('off')\n",
      "ax2.set_title('Canny filter, $\\sigma=1$', fontsize=20)\n",
      "\n",
      "ax3.imshow(edges2, cmap=plt.cm.gray)\n",
      "ax3.axis('off')\n",
      "ax3.set_title('Canny filter, $\\sigma=3$', fontsize=20)\n",
      "\n",
      "fig.subplots_adjust(wspace=0.02, hspace=0.02, top=0.9,\n",
      "                    bottom=0.02, left=0.02, right=0.98)\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Perform a Hough Transform\n",
      "# The accuracy corresponds to the bin size of a major axis.\n",
      "# The value is chosen in order to get a single high accumulator.\n",
      "# The threshold eliminates low accumulators\n",
      "result = hough_ellipse(edges, accuracy=20, threshold=250,\n",
      "                       min_size=100, max_size=120)\n",
      "result.sort(order='accumulator')\n",
      "\n",
      "# Estimated parameters for the ellipse\n",
      "best = result[-1]\n",
      "yc = int(best[1])\n",
      "xc = int(best[2])\n",
      "a = int(best[3])\n",
      "b = int(best[4])\n",
      "orientation = best[5]\n",
      "\n",
      "# Draw the ellipse on the original image\n",
      "cy, cx = ellipse_perimeter(yc, xc, a, b, orientation)\n",
      "image_rgb[cy, cx] = (0, 0, 255)\n",
      "# Draw the edge (white) and the resulting ellipse (red)\n",
      "edges = color.gray2rgb(edges)\n",
      "edges[cy, cx] = (250, 0, 0)\n",
      "\n",
      "fig2, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(8, 4))\n",
      "\n",
      "ax1.set_title('Original picture')\n",
      "ax1.imshow(image_rgb)\n",
      "\n",
      "ax2.set_title('Edge (white) and result (red)')\n",
      "ax2.imshow(edges)\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndexError",
       "evalue": "index -1 is out of bounds for axis 0 with size 0",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-18-e51a6d326872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Estimated parameters for the ellipse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0myc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mxc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for axis 0 with size 0"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}